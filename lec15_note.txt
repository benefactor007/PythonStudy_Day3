Lecture 15

Q: How many samples are needed to have confidence in result?
A: Fortunately, there is a very solid sef of mathematics that lets us answer this question in a good way.

>> At the root of all of it is the notion of variance.

** Variance is a measure of how mush spread there is in the possible outcomes.

In order to talk about variance, given this definition, we need to have different outcomes(plural), which is why we
always want to run multiple trails rather than do sth(say on trial with many flips).

Q: Why would I do multiple trails adding up to a million rather than 1 trail of a million?
A: The reason is by doing sth(having multiple trails, each of sth(which give me a different outcome), I can then look
at how different (the outcomes of the different trials) are and get a measure of variance.

>>> If I do 10 trails and get 10 wildly different answers, then I probably shouldn't believe any one of those answers,
and I probably shouldn't even think I can average those answers and believe the mean is a real answer because if run
an 11th trial maybe I'll get something totally different yet again.

** “Yet again” has a similar meaning to “once again”. You would say it when something that happens regularly happens.

>> we can formalize this notion of variance in a way that should be familiar to many of you.

the concept of Standard deviation - the fraction of vales close to mean

σ(x) = sqrt(1/abs(x)*Σ(x∈X)*(x-μ)^2), μ = mean

# We can say the standard deviation of x, where x is a set of trails, is equal to the square root of 1 over the
absolute value of (the length of x) the number of trails times the summation of the value of each trial, little x and
big X, of x minus μ(mu) squared, where μ(mu) is the mean.

>> sigma is usually used to talk about that.
>> As I said, that's the {cardinality[ˌkärdəˈnalədē]基数: the number of elements in a set or other grouping, as a
property of that grouping.}of x

Those of you (are majoring in math) are going to love that.

>> But for those of you who are more computationally(adv.) oriented. --> academically oriented.
I recommend you just take a look at teh code.
mean
Now we know what standard deviation means. What are we going to do with it?
We're going to use it to look at the relationship between the number of samples (we've looked
at) and how much confidence we should have in the answer.

Add on Tuesday afternoon, Dec,21.

Figure(3)
>> We expect the ratio to get smaller, but we expected the mean difference to get bigger.

** What I have to ask is not is the standard deviation large or small, but is it a relatively large or
a relatively small.
** Relative to the mean.
e.g. If the mean is a million, and the standard deviation is 20. It's a relatively small standard deviation.

>> It's pretty packed around the mean.
{packed: (of a room, building, or other place) filled by a large number of people; very crowded.}

>> The good news is we have again, a mathematical for formula that lets us do that.
>> Get rid of all those figures for the moment.

** The formula is called the coefficient of variation.

coefficient [ˌkōəˈfiSHənt]: a numerical or constant quantity placed before and multiplying the variable in
an algebraic[ˌaljəˈbrāik] expression (e.g. 4 in 4x y).

algebra: 代数学


>> For reasons I don't fully understand, this is typically not used.
People always talk about the standard deviation. But in many cases, it's the coefficient of variation that
really is a more useful measure.

** It's simply the standard deviation divided by the mean.

{Vary[ˈverē]: differ in size, amount, degree, or nature from something else of the same general class.}

How much they vary relative to each other.

** If < 1, that as low variance.
*** There should be some warnings that come with the coefficient of variance. And there are some of the
reasons people don't use it as often because they don't want to bother giving the warning labels.
*** If the mean is near 0, small changes in the mean are going to lead to large changes in the coefficient
of variance. They're not necessarily very meaningful.

*** So when the mean is near 0, the coefficient of variation is something you need to think about with
    several[ˈsev(ə)rəl] grains of salt.

"(With) a grain[ɡrān] of salt", (or "a pinch of salt") is an idiom[ˈidēəm] of the English language, which means to
view something with scepticism or not to interpret something literally.

pinch[pin(t)SH]: grip[ɡrip] (something, typically someone's flesh) tightly and sharply between finger and thumb.
grain[ɡrān]
scepticism[ˈskeptəˌsizəm]: a skeptical attitude; doubt as to the truth of something.
skeptical[ˈskeptək(ə)l]: not easily convinced; having doubts or reservations.

>> You're dividing by something near 0, a small change is going to produce something big.

Perhaps more importantly, or equally importantly and this is something we're going to talk about later
is that unlike the standard deviation,
** the coefficient of variation cannot be used to construct confidence {intervals[ˈin(t)ərvəl]: a space between two
things; a gap }.

>> I know we haven't talked about confidence intervals yes, but we will shortly.

tremendously[trəˈmendəslē]: to a very great extent.

>> You've got to be tremendously bored with flipping coins.
Nevertheless, I'm going to ask sb(you) to do(look at) sth (one more coin flipping simulation.)
And then I promise we'll change the topic.

>> This is to show you some more aspects of the plotting facilities in PyLab.

>> You've(You have) seen this zillion times.

>> use fairly frequently.
>> Some other things I want to show you here is I'm using xlim and ylim
What we could do here is this is setting the limits of the x and y-axis, rather than using defaults.

Q: How to lie with data?
A: (A great way to fool people with figure) is to {subtly[ˈsəd(ə)lē]:in a clever and indirect way, in order to achieve
something.} change the range of one of the axes.

>> When in fact, neither conclusion is ture.
It's just that they've(they have) been normalized to either look the same or look different.

"Let's run it and see what we get"

>> Much easier to prepare than actual material, but nevertheless, shouldn't take forever.
>> I should probably be testing all these things out on the slow computer before making you wait.

>> Let's look at the two figures side by side.

squeeze[skwēz]: firmly press (something soft or yielding), typically with one's fingers.

>> make them a little smaller(adj.) so we can squeeze them both in.

Q: What have we got here?

>> It says, as we've discussed, that these results(plural)(1,000 flips) are more credible than
these results (100 flips).
Not to day that they're more accurate because they're not really, but they're more believable.
Notice also the spread of outcomes is much tighter here

tight[tīt]: fixed, fastened, or closed firmly; hard to move, undo, or open.
** That's why I played with xlim. If I used the default values, it would not have looked much tighter
when I put this up on the screen because it would have said well, we don't have any values out here.
I don't need to display all of this(singular).
It would have then about the same visual width[widTH] as this.
>> And therefore, potentially very deceptive when you just stared at it if you didn't look carefully at the
units on the x-axis

deceptive[dəˈseptiv]: giving an appearance or impression different from the true one; misleading.

stare[ster]: look fixedly or {vacantly[ˈvākənt]: empty} at someone or something with one's eyes wide open.

>> (what I did) is since I knew I wanted to show you these things side by side and make the point about
how tight the distribution is, I made both axes run the same length.

** to produce comparable figures.
>> to put the text box in a place where it would be easy to see.
>> You can also use the fault of best, which often puts it in the right place, but not always.

** The distribution of the results in both cases is close to something called the normal distribution.

>> We are talking about not just the average value but the distribution of values in these trials.

which is very common.

** The normal distribution, which is very common, has some interesting properties.
*** It always peaks at the mean and falls off symmetrically.

symmetrical[səˈmetrik(ə)l]: made up of exactly similar parts facing each other or around an axis; showing symmetry.
symmetry[ˈsimətrē]: noun.

The shape of the normal distribution.
There are people who imagine it looks like a bell. And Therefore, the normal distribution is often also called
the bell curve.

>> And indeed, mathematicians[ˌmaTH(ə)məˈtiSHən] will always call it this.
This is often what people use in the non-technical literature.
>> There was, for example, a very controversial book called "The Bell Curve", which I don't recommend doing(reading.)

controversial[ˌkäntrəˈvərSHəl]: giving rise or likely to give rise to public disagreement.

It's not really exactly symmetric.

Add on Wednesday, Dec. 22
>> If I did 100,000 trials of a 100,000 flips each, we wouldn't finish the lecture.
I'd take too long.


** Normal distribution are frequently used in constructing probabilistic[ˌpräbəbəˈlistik] models for two reasons.
*** 1) Reason one, is they have nice mathematical properties. They're easy to reason about for
    reasons we'll see shortly.
>> The curve (where every value is the same) has even nicer mathematical properties but isn't very useful.
*** 2) Many naturally occurring instances.

reason: verb. think, understand, and form judgments by a process of logic.
e.g. humans do not reason entirely the facts.

>>> That's not good enough.
>> Let's first look at what makes them nice mathematically and then let's look at where they occur.
They can be completely characterized[ˈkerəktəˌrīz] by two parameters, the mean and the standard deviation.

>> Knowing these is the equivalent to (is similar to) knowing the entire distribution.
Furthermore, if we have a normal distribution, the mean and the standard deviation can be used to compute confidence
intervals.

This is a very important concept.

>> One that you see all the time in the popular press but maybe don't know what it actually means when you see it.
So instead of estimating an unknown parameter, all we've been doing with this whole probability business.
>> You get some unknown parameter like the probability of getting a head or a tail, and we've been
estimating it using the various techniques.

>> Typically, you've been estimating it by a single value, the mean of a set of trials.

*** A confidence interval instead allows us to estimate the unknown parameter by providing a range that is likely to
contain the unknown value.

>> ... lie within ...(that range)

*** A confidence that the unknown value lies within that range. It's called the confidence level.

e.g. when you look at political[pəˈlidək(ə)] polls[pōl], you might see something that would say he candidate is likely
to get 52% of the vote plus or minus 4%(52% +- 4%)

Q: What does that mean?
A: If somebody doesn't specify the confidence level, they usually mean 5%.
>> So what this says is that 95% percent of the time, 95th confidence interval. If the election were actually conducted,
the candidate[ˈkandiˌdāt] would get somewhere between 48% to 56% of the vote.

So 95% percent of the time, 95% percent of the elections, the candidate would get between 48% and 56% of the votes.
>> We have two things, the range and our confidence that the value will lie within that range.

They are assuming that elections are random trails that have a normal distribution.
That's an implicit assumption in the calculation that tells us this.

** /ē/ : ee
e.g.  ee as in green; ea as in beach; e-e as in athlete[ˈaTHˌlēt]; y as in happy
implicit[imˈplisit]: implied though not plainly expressed.
plainly[ˈplānlē]: able to be perceived easily.
perceive[pərˈsēv]: become aware or conscious of (something); come to realize or understand.


poll: the process of voting in an election.
political: relating to the government or the public affairs[əˈfer] of a country.
affair[əˈfer]: an event or sequence of events of a specified kind or that has previously been referred to.
be conducted[kənˈdəktəd]: the action or manner of managing an activity or organization.
candidate: a person who applies for a job or is nominated for election.

>> instead do sth (instead allow sb to do sth)

popular press: Popular press sources are magazines, newspapers, and books intended to inform or entertain general
audiences.

** There is something called the empirical[əmˈpirik(ə)l] rule, which is used for normal distributions.
darn close
empirical[əmˈpirik(ə)l]: based on, concerned with, or verifiable by observation or experience rather than theory or pure
 logic.

Empirical rule: They give us a handy way to estimate confidence intervals given the mean and the standard deviation.
e.g. If we have a true normal distribution, then roughly[ˈrəflē] speaking, 68%(not exactly) of the data are within the
one standard deviation of the mean. 95%(not exactly) percent with two standard deviations, and almost all, 99.7% will
fall within three stand deviations.

>> These values are approximations.[əˌpräksəˈmāSH(ə)n], and they're not exactly right.
In fact, they're good enough for government work.

>> This is what people use when they think about these things.
>> sth(this) may raise an interesting question in your mind.

Q: How do the pollsters go about finding the standard deviation?
Q2: Do they go out and conduct a 100 separate polls and then do some math.

>> sort of (kind of) what we've been doing.
>> You might hope so, but that is not (what they do) because it's expensive.
>> Nobody wants(Third-Person Singular Verb) to do that.
They use another trick to estimate the standard deviation. Now, you're beginning to understand why these polls aren't
always right.

** The trick they use for that is something called the standard error, which is an estimate of the standard deviation.
You can only do this under the assumption that the errors are normally distributed and also that the sample population
is small(It's small relative to the actual population).

>> This gets sb(us) to one of the things we like about the normal distribution that in fact, it's often an accurate
model of reality. When people have done polls over and over again, they do discover that, indeed, the results are
typically normally distributed. So this is not a bad assumption

>> ... can only do...
>> roughly speaking
handy[ˈhandē]: convenient[adj.] to handle or use; useful.

Add on Thursday, Dec. 23

>> if we heave p, which is equal to the percentage sample. And we have n, which is equal to the sample size.
We can say that the standard error which I'll write SE(standard error), is equal to the formula,

SE = (p * (100 - p)/n) ** 0.5
** Warning: p = %. Have to format the p as percentage.
P times (100 minus p) divided by n to the 1/2(one half)

For example, a pollster were to sample 1,000 voters, and 46% of them said that they'll vote for sb.
The standard error would be roughly 1.58%

>> be within: inside (something).

We would interpret this to mean that in 95% of the time, the true percentage of votes that sb would get is within
two standard errors of 46%.

>> I know that's a lot to swallow quickly.

swallow[ˈswälō]: cause or allow (something, especially food or drink) to pass down the throat[THrōt].

>> As always, we'll try and make sense of it by looking at some code.

Add on Friday, Dec. 24

By now, you've probably all figured out that I'm much more comfortable with code than I am with formulas.

>> we're going to conduct a poll here.
Not really, we're going to pretend we're conducting a poll.
we'll start with no votes.

>> Otherwise, it will stay where it was and will return the number of votes.
Nothing very dramatic.
>> let's see what we get when we do this.

A Note on Notation
Notation: a series or system of written symbols used to represent numbers, amounts, or elements in something such as
music or mathematics.

Some of the confusion surrounding this subject is due to the notation used. People sometimes call the standard deviation
of a set of numbers the "population standard deviation", and write it as σ(n). (It is often called σn on calculators.
The notation makes sense because, when finding the variance, we divide by n.) They then sometimes refer to the corrected
quantity, √(n/(n-1)) times this, the estimate of the true population standard deviation if what we had was a sample,
as the "sample standard deviation", and write it as σn-1. The σn-1 notation at least reminds us what we're doing, but
calling it the sample standard deviation is potentially confusing, because it's not the standard deviation of the
sample. Instead, it's an estimate of the population standard deviation if what we had was a sample.

In more serious work, the true, unknown variance of the population is usually denoted σ2, and the actual variance of
the sample is usually denoted s2. Thus what Bessel's correction tells us is that an unbiased estimator for σ2 is given
by (n/(n-1))s2. I'll use this notation in subsequent sections. I'll also use μ for the true, unknown mean of the
population, and x for the actual mean of the sample.


>> Pretty darn close: really close
e.g. Pretty darn[därn] close to a normal distribution.
>> Kind of what we'd expect.
The fraction of votes peaks at 46%, again what we'd expect.
>> every once in while: sometimes but not often : from time to time : occasionally[əˈkāZH(ə)nəlē]
But every once in while, it gets all the way out here to 50.
Highly unlikely in our modern society.
So it turns out that the standard error, which you'll remember we computed using that formula to be 1.58.

remember the standard error is an attempt to just use a formula to estimate what the standard deviation is going to be.

n = {int} 1000
p = {float} 0.46
se = ((46*(100-46))/n)**0.5
se = {float} 1.57607
** The unit of se is percentage.

use this simple formula to guess what it would be.
>> We then ran a simulation and actually measured the standard deviation, no longer a guess.
And it came out to be 1.6

I hope that most of you would agree that was pretty good guess.
Therefore, if you will, the differences are normally distributed, the distribution is normal.
>> It turns out the standard error is a very good approximation to the actual standard deviation.
>>> That's what pollsters rely on and why polls are actually pretty good.

Now the next time you read a poll, you'll understand the math behind it.
In a subsequent lecture, we'll talk about some ways they go wrong that have nothing to do with getting the
math wrong.

be (of) no use: to not be useful
tractable[ˈtraktəb(ə)l]: adj. (of a person) easy to control or influence.

Finding a nice tractable mathematical model, the normal distribution, is of no use if it provides an inaccurate
model of the data that you care about.

Fortunately, many random variables have an approximately normal distribution.

For example, I were doing a real lecture and I had 100 students in this room, and if I were to look at the
heights of the students, we would find that they are roughly normally distributed.


FORM
***[If ... were to + verb ..., ...]

USE
Were to can be used in the present to emphasize that the conditional form is extremely unlikely or unthinkably horrible.
Notice that this special form is only used in the if-clause[klôz].

** do end up doing sth.

Any time you take a population of people and you look at it, it's quite striking that you do end up doing(getting)
a normal distribution of the heights.

You gets a normal distribution of the weights[wāt].


striking[ˈstrīkiNG]: adj. attracting attention by reason of being unusual, extreme, or prominent[ˈprämənənt].

>> Same thing will be true if you look at plants, all sorts of things(plural) like that.

As I known == What I do know

>> What I do know is that and probably this is more important.
>> Many experimental setups, and this is what we're going to be talking about going forward, have normally
distributed measurement errors.

This assumption was used first in the early 1800s (eighteen-hundreds) by the German mathematician[ˌmaTH(ə)məˈtiSHən]
and physicist[ˈfizəsəst] Carl Gauss.

You've probably heard of Gauss, who assumed a normal distribution of measurement errors in his analysis of
astronomical[ˌastrəˈnämək(ə)l] data. He was measuring various things in the heavens. He knew his measurements of
(where something was) were not 100% accurate. And he said, well,
>> I'll bet it's equally likely (it's) to the left of where I think it is or the right as where I think it is.
I'll bet the further I get from its true value, the less likely I am to guess that's where it is.

He invented at that time what we now call the normal distribution.

astronomical: relating to astronomy[əˈstränəmē].
astronomy: the branch of science which deals with celestial[səˈlesCHəl] objects, space, and the physical universe as a
whole.

>> Physicists insist today still on calling it a Gaussian distibution.
And it turned out to be a very accurate model of measurement errors he would make.

e.g. If you guys are in a chemistry lab, or a physics lab, or a bio. lab, mechanical engineering lab, any lab
where you're measuring things, it's pretty likely that the mistakes you will make will be normally distributed.

unsystematic: adj. not done or acting according to a fixed plan or system; unmethodical[ˌənˌməˈTHädək(ə)l].
sloppy[ˈsläpē]:careless and unsystematic; excessively casual.
casual[ˈkaZHo͞oəl]: relaxed and unconcerned.

>> It's not just because you were sloppy in the lab. Actually, if you were sloppy in the lab, they may not be
normally distributed.

If you're not sloppy in the lab, they'll be normally distributed.
>> It's true of almost all measurements.
In fact, most of science assumes normal distributions of measurement errors in reaching(getting) conclusions about
the validity[vəˈlidədē] of their data.

>> we'll see some examples of that as we go forward